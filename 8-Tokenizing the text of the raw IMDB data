#Tokenizing the text of the raw IMDB data

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np

maxlen = 100 #cuts off after 100 words

training_samples = 200 #trains on 200 samples

validation_samples = 10000 #validates on 10000 samples

max_words = 10000 #considers only the top 10000 words in the dataset

tokenizer = Tokenizer(num_words=max_words) #tokenizes the words

tokenizer.fit_on_texts(texts)

sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index #tokenize words

print('Found %s unique tokens.' % len(word_index))

data = pad_sequences(sequences, maxlen=maxlen) #padding is added

labels = np.asarray(labels)

print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

#splits the data into training set and a validation set
indices = np.arange(data.shape[0]) 

#shuffles the data because data is ordered
np.random.shuffle(indices)

data = data[indices]

labels = labels[indices]

x_train = data[:training_samples]
y_train = labels[:training_samples]

x_val = data[training_samples: training_samples + validation_samples]
y_val = labels[training_samples: training_samples + validation_samples]
