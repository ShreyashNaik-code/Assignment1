# Word-level one-hot encoding with hashing trick

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

dimensionality = 1000 #stores the words as vector size 1000

max_length = 10 #cuts off text after max length

results = np.zeros((len(samples), max_length, dimensionality))

for i, sample in enumerate(samples):

  for j, word in list(enumerate(sample.split()))[:max_length]:

    index = abs(hash(word)) % dimensionality #hashes the word into a random integer index between 0 and 1000

    results[i, j, index] = 1
