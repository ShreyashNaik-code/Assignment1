#Go to https://nlp.stanford.edu/projects/glove, and download the precomputed
#embeddings from 2014 English Wikipedia. Itâ€™s an 822 MB zip file called glove.6B.zip,
#containing 100-dimensional embedding vectors for 400,000 words (or nonword
#tokens). Unzip it.

#Parsing the GloVe word-embeddings file

glove_dir = '#'

embeddings_index = {}

f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))

for line in f:
  values = line.split() #splits the line from the .txt file
  word = values[0]
  coefs = np.asarray(values[1:], dtype='float32')
  embeddings_index[word] = coefs
f.close()
print('Found %s word vectors.' % len(embeddings_index))

#Preparing the GloVe word-embeddings matrix

embedding_dim = 100

#Return a new array of given shape and type, filled with zeros.
embedding_matrix = np.zeros((max_words, embedding_dim)) 

for word, i in word_index.items():
  if i < max_words:
    embedding_vector = embeddings_index.get(word)
      if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector #words not found in the embedding index will be all zeros
 
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()


#Loading pretrained word embeddings into the Embedding layer

model.layers[0].set_weights([embedding_matrix])

model.layers[0].trainable = False #freeze the Embedding layer 

#Training and evaluation

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])

history = model.fit(x_train, y_train,
                    epochs=10,
                    batch_size=32,
                    validation_data=(x_val, y_val))
model.save_weights('pre_trained_glove_model.h5')
