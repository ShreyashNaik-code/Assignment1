#Word-level one-hot encoding
import numpy as np
samples = ['The cat sat on the mat.', 'The dog ate my homework.'] #initial data input

token_index = {} #builds an index of all tokens in data

for sample in samples: #takes input from samples
  for word in sample.split(): #splits the words 
    if word not in token_index: #checks for duplicate words
      token_index[word] = len(token_index) + 1 #assigns unique number to words

max_length = 10 #cuts off text after max length

results = np.zeros(shape=(len(samples),
                           max_length,
                           max(token_index.values()) + 1)) #stores result
for i, sample in enumerate(samples):
  for j, word in list(enumerate(sample.split()))[:max_length]:
    index = token_index.get(word)
    results[i, j, index] = 1
